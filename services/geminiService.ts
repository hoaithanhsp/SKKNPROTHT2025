
import { GoogleGenAI, Chat } from "@google/genai";
import { SYSTEM_INSTRUCTION, FALLBACK_MODELS } from "../constants";

let chatSession: Chat | null = null;
let currentApiKey: string | null = null;
let currentSelectedModel: string | null = null;
let history: any[] = []; // Store history to restore when switching models

export const initializeGeminiChat = (apiKey: string, selectedModel?: string) => {
  currentApiKey = apiKey;
  currentSelectedModel = selectedModel || FALLBACK_MODELS[0];
  chatSession = null;
  history = []; // Reset history on new initialization
};

const createChatSession = (model: string) => {
  if (!currentApiKey) throw new Error("API Key not found");

  const ai = new GoogleGenAI({ apiKey: currentApiKey });

  return ai.chats.create({
    model: model,
    config: {
      systemInstruction: SYSTEM_INSTRUCTION,
      temperature: 0.7,
      topK: 64,
      topP: 0.95,
      maxOutputTokens: 65536,
      thinkingConfig: { thinkingBudget: 2048 },
      tools: [{ googleSearch: {} }]
    },
    history: history
  });
};

// S·∫Øp x·∫øp models v·ªõi model ƒë∆∞·ª£c ch·ªçn ƒë·∫ßu ti√™n
const getOrderedModels = (): string[] => {
  if (!currentSelectedModel || !FALLBACK_MODELS.includes(currentSelectedModel)) {
    return FALLBACK_MODELS;
  }

  // ƒê∆∞a model ƒë∆∞·ª£c ch·ªçn l√™n ƒë·∫ßu, gi·ªØ nguy√™n th·ª© t·ª± c√°c model c√≤n l·∫°i
  const orderedModels = [currentSelectedModel];
  for (const model of FALLBACK_MODELS) {
    if (model !== currentSelectedModel) {
      orderedModels.push(model);
    }
  }
  return orderedModels;
};

export const sendMessageStream = async (message: string, onChunk: (text: string) => void) => {
  if (!currentApiKey) throw new Error("API Key not initialized");

  let lastError: any = null;
  const modelsToTry = getOrderedModels();

  // Try through the fallback models
  for (const model of modelsToTry) {
    try {
      console.log(`ü§ñ ƒêang th·ª≠ model: ${model}`);

      // Always recreate session with current history to ensure we use the selected model
      // (or optimize to reuse if same model, but recreation is safer for fallback)
      chatSession = createChatSession(model);

      const responseStream = await chatSession.sendMessageStream({ message });

      let fullResponse = "";
      for await (const chunk of responseStream) {
        if (chunk.text) {
          onChunk(chunk.text);
          fullResponse += chunk.text;
        }
      }

      // If successful, update history and break
      history.push({ role: 'user', parts: [{ text: message }] });
      history.push({ role: 'model', parts: [{ text: fullResponse }] });
      console.log(`‚úÖ Model ${model} th√†nh c√¥ng!`);
      return;

    } catch (error: any) {
      console.error(`‚ùå Model ${model} th·∫•t b·∫°i:`, error.message || error);
      lastError = error;
      // Continue to next model
    }
  }

  // If all models fail
  throw lastError || new Error("T·∫•t c·∫£ models ƒë·ªÅu th·∫•t b·∫°i. Vui l√≤ng ki·ªÉm tra API key ho·∫∑c th·ª≠ l·∫°i sau.");
};
